---
title: "Untitled"
author: "Reza Dwi Utomo"
date: "28/03/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction {#intro}

## Background

This article aims to accomplish [Machine Learning Specialization Capstone Project](https://algorit.ma/machine-learning-specialization/) course at Algoritma. The dataset used is specially obtained from [here](https://rpubs.com/AlgoritmaAcademy/ml-capstone). You could see the source code fully in my GitHub account [here](https://github.com/utomoreza/FnB).

All dataset used in capstone could be found in this [link](https://drive.google.com/drive/folders/18czGYmVyAngP1OmxUquGOWkOJldc1Y6B?usp=sharing). In the link, it is provided for each case 2 datasets: train and test dataset.

The train dataset will be used to train and evaluate the model, while the test dataset is used as the final evaluation and requires you to submit the prediction to the [leaderboard](https://algoritma.shinyapps.io/leaderboard_capsml/) in order to obtain the model evaluation. The data scheme is illustrated as follows:

![](index.png)

## Content {#content}

The Food and Beverage dataset is provided by Dattabot, which contains detailed transaction of multiple food and beverage outlets. Using this dataset, we are challenged to do some forecasting and time series analysis to help the outletâ€™s owner making a better business decision.

Customer behaviour, especially in food and beverage industry, is highly related to seasonality patterns. The owner wants to analyze the number of visitors so he could make better judgement in 2018. Fortunately, we already know that time series analysis is enough to provide a good forecast and seasonality explanation.

The dataset includes information about:

+ `transaction_date`: The timestamp of a transaction
+ `receipt_number`: The ID of a transaction
+ `item_id`: The ID of an item in a transaction
+ `item_group`: The group ID of an item in a transaction
+ `item_major_group`: The major-group ID of an item in a transaction
+ `quantity`: The quantity of purchased item
+ `price_usd`: The price of purchased item
+ `total_usd`: The total price of purchased item
+ `payment_type`: The payment method
+ `sales_type`: The sales method

## Aim {#aim}

Here, we are asked to make a report of forecasting result and seasonality explanation for hourly number of visitors, that would be evaluated on the next 7 days (Monday, 19 December 2017 to Sunday, 25 December 2017).

## Objectives

+ %To aggregate the dataset in order to obtain `visitor` feature.
+ %To round `time` feature in respective hour.
+ %To perform time series padding for incomplete time series.
+ %To replace missing values in time series with more meaningful values.
+ To create time series object.
+ To decompose the time series object into trend, sesonality, and residual.
+ To visualize the observed seasonality.
+ To interpret the observed seasonality.
+ To examine assumption checking, i.e. autocorrelation, normality, and stationarity.
+ To perform cross validation of the train dataset.
+ To model the time series object and perform model fitting.
+ To evaluate the model and vizualize it.
+ To compare several models.
+ To reach MAE (Mean Absolute Error) less than 6 in validation dataset.
+ To forecast the test dataset using the final model.
+ To reach MAE less than 6 in test dataset.

## Structure

This article is arranged as follows.

1. [Introduction](#intro)
2. [Preparation](#prepar)
3. [Exploratory Data Analysis](#eda)
4. [Preprocessing](#preproc)
5. [Modelling and Evaluation](#model)
6. [Model Tuning](#tuning)
7. [Final Model](#final)
8. [Conclusions](#concl)

# Preparation {#prep}

Load all necessary packages.

```{r message=FALSE}
library(tidyverse) # to ease data cleansing, use ggplot
library(ggfortify) # to assist ggplot in plotting time series object
library(lubridate) # to process datetime variables
library(forecast) # to perform time series modelling
library(MLmetrics) # to evaluate the models using MAE
library(TSstudio) # to plot the forecasted results
library(tseries) # to use adf.test (staionarity test)
```

Load the train dataset and view its first five rows.

```{r}
fnb <- read.csv("data/data-train.csv")
head(fnb)
str(fnb)
```

As stated earlier in the [Aim](#aim) section above, we're going to perform time series analysis. Therefore, looking at above data frame, we can see there are many unnecessary features. For time series, we're only interested in two features, i.e. time and the feature in forecasting question (which is visitor).

However, the data frame above does not have the visitor feature we need. Thus, we have to create it ourselves. From all variables available in the data frame, we can see `transaction_date` contains date-time values that we're going to use it as date-time feature. However, to find good feature candidate for visitor feature, we have to carefully look again to and read the [Content](#content) section. There, we can see that `receipt_number` could be a good candidate as it represents the ID of every single transaction. And every transaction means every customer. If we accurately look at the data frame, each `item_id` could have the same `receipt_number` as other `item_id`. Since `item_id` represents The ID of an item in a transaction, it is clear that every `receipt_number` could have more than one items or `item_id`. In other words, every customer **or visitor** can order more than one food and/or drink. Therefore, we agree to select `receipt_number` as the candidate to create `visitor` feature.

Now, let's drop all unnecessary variables, except `transaction_date` and `receipt_number`.
Next, since the `transaction_date` column is not in correct data type as shown by `str()` function above, we need to change its data type to date-time by using `ymd_hms()` function from the `lubridate` library.

```{r}
fnb <- fnb %>% select(transaction_date, receipt_number)
fnb$transaction_date <- ymd_hms(fnb$transaction_date)
head(fnb)
```

We've obtained necessary columns. If we show their first 6 rows, all date-time values have values until minutes and seconds. We need to round them in their hour in order to facilitate us in counting the visitors for each hour. We can use the `floor_date` function from `lubridate` and set the unit parameter to `"hour"` as we want to round them in hour. Afterwards, we can group all rows in the `transaction_date` column in order to count the number of unique `receipt_number` (since every single `receipt_number` means every single visitor). From such counted values, we can create a new column called `visitor`.

```{r}
fnb <- fnb %>% 
  mutate(transaction_date = floor_date(x = .$transaction_date, unit = "hour")) %>% 
  group_by(transaction_date) %>% 
  summarise(visitor = n_distinct(receipt_number)) %>% 
  ungroup()
fnb
```

Next, if we look at the data frame above, the `transaction_date` column does not have full 24 hours time series. From the beginning of hour `13:00:00`, its irregularity starts. Sometimes, the hour of a day begins at 10 AM, 9 AM, or 1 PM. Hence, we have to solve this irregularity by unifying every single day with 24 hours, i.e. every single day starting at 0 AM and ending at 23 PM. This is called time series padding. We can do this by using the `pad` function from the `padr` library as shown below.

```{r}
min_date <- min(fnb$transaction_date) # define the last date
max_date <- max(fnb$transaction_date) # define the first date

# we make the first date-time value. Since the min_date contains hour value in 1 PM, we dont want this so that we have to create an hour of "00:00:00"
start_val <- make_datetime(year = year(min_date), month = month(min_date), day = day(min_date), hour = hour(hms("00:00:00")))

# we make the last date-time value
end_val <- make_datetime(year = year(max_date), month = month(max_date), day = day(max_date), hour = hour(max_date))

# we perform time series padding and assign the results in a new variable called fnb_pad
fnb_pad <- padr::pad(x = fnb, start_val = start_val, end_val = end_val)

# show the data frame
fnb_pad
```

From the data frame shown above, we can see a lot of missing values. To see them clearly, let's count them.

```{r}
is.na(fnb_pad) %>% colSums()
```

There are 676 missing values in the `visitor` column. This is unsurprised results as we have carried out time series padding, then the blank rows in `visitor` columns are filled with missing values. We need this missing values because basically this represents the absence of visitor, namely 0 visitor. Therefore, instead of filling with NAs, we'd like to fill such missing values with 0 value.

```{r}
fnb_pad$visitor <- replace_na(fnb_pad$visitor, 0)
fnb_pad
is.na(fnb_pad) %>% colSums()
```

All NAs have been changed to 0 value. All clean. The dataset now is ready for exploratory data analysis.

# Exploratory Data Analysis {#eda}

## Visualize the Dataset {#vizdata}

In this chapter, we're going to explore and understand more the dataset. We could do that by visualizing its distribution of number of visitors in every hour and day. To do that, firstly, we need to make two new columns, i.e. hour and day columns.

```{r}
fnb_pad$hour <- hour(fnb_pad$transaction_date) # create the hour column
fnb_pad$day <- wday(x = fnb_pad$transaction_date, label = T, abbr = F) # create the day column
fnb_pad
```

By using just created columns and the `visitor` column, we can plot the distribution of the dataset as shown below. We also could utilize the `ggplotly` function from the `plotly` library in order to make the plot interactive.

```{r}
plotly::ggplotly(fnb_pad %>% ggplot(aes(x = hour, y = visitor)) +
                   geom_col(aes(fill = day)) +
                   theme_minimal() +
                   labs(title = "Number of visitors",
                        x = "Hour",
                        y = "Number of visitors",
                        fill = "Day"))
```

As shown in the plot above, at a glance, it can be seen that between 2 AM and 9 AM, there is no visitor. However, as mentioned earlier in the [Preparation](#prep) section, exactly in the part before implementing time series padding, there exist a day which starts at 9 AM. Therefore, if we zoom in the plot above to the part between 9 AM and 10 AM, a day which starts at 9 AM can be seen. Uniquely, such a day only has one visitor.

In addition, the plot above also indicates that the nighter a day, the more the visitors come. Nevertheless, this phenomenon only applies from the opening time to 8 PM since after 8 PM, the number of visitors gradually decreases. 

For the proportion of the number of visitors, it is approximately balanced among each day of week. However, to make it clear, let's compare each day of week by using boxplots.

```{r}
fnb_pad %>% ggplot(aes(x = day,
                       y = visitor)) +
  geom_boxplot(fill = "green") +
  geom_jitter(aes(color = hour)) +
  labs(title = "Boxplots of each day",
       x = "Day",
       y = "Number of visitors",
       color = "Hour")
```

From the boxplots above, it can be seen that weekend (Saturday and Sunday) relatively has more visitors than weekday. It is clear that the median of Saturday and Sunday is higher than that of weekday. From Monday to Thurday, their median is relatively the same. However, something weird happens when we see the median of Friday. Such day has the least median value while its distribution is wider than other weekdays. This indicates that Friday tends to form random pattern.

## Visualize the Time Series Object (1 Seasonal) {#1seas}

Subsequently, we're going to visualize the dataset in time series object. To do this, first of all, we need to create a time series object by using the `ts()` function. The column in interest is the `visitor` column. Since the dataset is in hourly observations, we can set the `frequency` parameter to 24 (i.e. 24 hours) to become a single day. Then, let's plot the time series object.

```{r}
fnb_ts <- ts(data = fnb_pad$visitor, frequency = 24)
autoplot(fnb_ts)
```

After the time series object created, now let's decompose it to see its composition (i.e. sesonality, trend, and residual/remainder) by using the `decompose()` function. Then, let's plot the decomposed time series object.

```{r warning=FALSE}
fnb_dec <- decompose(x = fnb_ts, type = "additive")
fnb_dec %>% autoplot()
```

It can be seen above that the trend still has patterns. This implicitly indicates that the time series object has more than one seasonality. We will cope with this issue little bit later. For now, just move on using 24-hour seasonal. Now, let's plot the seasonality.

```{r}
fnb_pad %>% 
  mutate(seasonal = fnb_dec$seasonal) %>% # create the seasonal column from decomposed ts object
  distinct(hour, seasonal) %>% # only show the unique values
  ggplot(mapping = aes(x = hour, y = seasonal)) +
  geom_col() +
  scale_x_continuous(breaks = seq(0,24,1)) +
  labs(title = "Seasonality Pattern Analysis",
       subtitle = "Hourly") +
  theme(axis.text.y = element_blank()) # we're not interested in seasonal values
```

The plot above shows the seasonality of 24 hours. Here, we only use one seasonality, i.e. from hours to day. Since the seasonality values in y-axis are uninterpretable, we're not interested to show the values. We're only interested in the bar changes. One thing that can be interpreted from the plot above is that every day the seasonality indicates a cycle. From 0 AM, there is significant decrease, and the values remain stable from 2 AM to 9 AM (although 9 AM shows a slight increase). From 10 AM to 8 PM, the values keep increase, whereas the values decrease afterwards. What the seasonality shows us is similar to what we've found in the [dataset visualization](#vizdata) above.

## Visualize the Time Series Object (2 Seasonals)

Next, as we've discovered earlier that, by using one seasonality, the trend still has patterns, it is recommended to add more seasonality to the time series object. Since our first seasonality is in hourly to day, now our second seasonality will be in daily to weekly, namely 24 hours to 7 days (i.e. 24 x 7).

The `msts()` function from the `forecast` library can be used to catch more than one seasonality. As now our seasonalities are in hourly to daily and daily to weekly (i.e. 24 and 24 x 7, respectively), we can set the `seasonal.periods` parameter for the function as 24 and 24 x 7. Then, in order to decompose the multiple seasonality, instead of using the `decompose()` function, we're going to use the `mstl()` function from the `forecast` library as well. After the decomposed multiple seasonality time series object created, we can plot it.

```{r}
msts(data = fnb_pad$visitor, seasonal.periods = c(24, 24*7)) %>% 
  mstl() %>% # decompose the multiple seasonality time series object
  autoplot() # plot the decomposed one

fnb_msts <- msts(data = fnb_pad$visitor, seasonal.periods = c(24, 24*7)) # save time series object
fnb_msts_dec <- mstl(fnb_msts) # save the decomposed one
```

By using the time series object with two seasonality, we're going to plot the first seasonality. We can do that by executing below codes.

```{r}
as.data.frame(fnb_msts_dec) %>% # change data type to dataframe as ggplot only accepts dataframe
  mutate(hour = fnb_pad$hour) %>% # create the hour column
  ggplot(mapping = aes(x = hour, y = Seasonal24)) + # Seasonal24 contains 1st seasonal
  geom_col() +
  labs(title = "Seasonality Pattern Analysis",
    subtitle = "Hourly") +
  theme(axis.text.y = element_blank()) # we're not interested in seasonal values
```

The plot above exactly shows the same results as the [plot](#1seas) of time series object with only one seasonality. Therefore, the interpretation of the plot is also the same.

Subsequently, let's plot the second seasonality.

```{r}
as.data.frame(fnb_msts_dec) %>% # change data type to dataframe as ggplot only accepts dataframe
  mutate(day = fnb_pad$day) %>% # create the day column
  ggplot(mapping = aes(x = day, y = Seasonal168)) + # Seasonal168 contains 2nd seasonal (24*7 = 168)
  geom_col() +
  labs(title = "Seasonality Pattern Analysis",
    subtitle = "Daily") +
  theme(axis.text.y = element_blank()) # we're not interested in seasonal values
```

The plot above shows something interesting. It is completely different from the plot of first/one seasonality. This second seasonality plot is rather similar to the boxplot we've created earlier in the [dataset visualization](#vizdata). What can be interpreted from the plot above is that weekend tends to have more significant numbers than weekday.

Now, let's combine both seasonalities and plot them together.

```{r}
# double seasonality plotted together

as.data.frame(fnb_msts_dec) %>% # change data type to dataframe as ggplot only accepts dataframe
  mutate(day = fnb_pad$day, # create the day column
         hour = fnb_pad$hour) %>% # create the hour column
  group_by(day, hour) %>% # group the data frame by day and hour
  summarise(seasonal = sum(Seasonal24 + Seasonal168)) %>% # then create new column called seasonal whose content is the combination between 1st and 2nd seasonality
  ggplot(mapping = aes(x = hour, y = seasonal)) +
  geom_col(aes(fill = day)) +
  scale_fill_viridis_d(option = "plasma") +
  labs(title = "Seasonality Pattern Analysis",
    subtitle = "Hourly and Daily") +
  theme(axis.text.y = element_blank()) # we're not interested in seasonal values
```

The plot above is similar to the plot of first/one seasonality, but with more descriptions in daily information. Therefore, its interpretation is similar as well, but with an addition of the second seasonality plot interpretation.

# Modelling and Evaluation {#model}

We've explored the dataset and its time series object. Now, let's move on to create the models and evaluate them. As explained earlier, we have two types of time series object, i.e. the one with one seasonality only and the one with two seasonalities. Here, we're going to examine both. First, let's examine the one with one seasonality only.

## Modelling of One Seasonality

Firstly, we should check the assumptions. For time series, usually, there are three assumptions, i.e. normality of residuals, no-autocorrelation, and stationarity. However, as we're going to employ triple exponential smoothing (Holt Winters) and ARIMA/SARIMA and as mentioned [here](https://stats.stackexchange.com/questions/64911/does-the-holt-winters-algorithm-for-exponential-smoothing-in-time-series-modelli) and [here](https://stats.stackexchange.com/questions/79400/does-arima-require-normally-distributed-errors-or-normally-distributed-input-dat), we won't consider to use normality of residuals. We will just use no-autocorrelation and stationarity from now on.

Now, let's check whether autocorrelation exists. We can use the `Box.test()` function. We expect to have p-value greater than 0.05.

```{r}
Box.test(fnb_dec$random)

diff(fnb_ts, lag = 2, differences = 9) %>% decompose(type = "additive") %>% 
  autoplot()

autoplot(diff(fnb_msts_dec[,5], lag = 2, differences = 3))

aa <- diff(fnb_msts, lag = 2, differences = 4) %>% mstl()
Box.test(aa[,4])
```

The p-value is too far from our expectation. It means that our 

```{r}

```


```{r}
adf.test(x = fnb_ts)
```

Now, let's split the time series object into two parts, i.e. the train data and the validation data. We don't use the term of test data here as we reserve it for the final test. Since the aim of this article is to forecast 7 days, we're going to create the validation data with length of 7 days as well (24 x 7).

```{r}
# split train-validation 1 seasonal
train <- head(fnb_ts, length(fnb_ts) - 24 * 7)
validation <- tail(fnb_ts, 24 * 7)
```

Now, let's create the model using triple exponential smoothing. We can use the `HoltWinters()` and `ets()` functions.

```{r}
model_holt<- HoltWinters(train, seasonal = "additive")

model_ets <- ets(train, model = "ZZZ")
```

Now, let's evaluate both models by forecasting the validation data and check their performance in MAE (Mean Absolute Error). Then, plot the results.

```{r}
forecast_holt <- forecast(model_holt, h = 24 * 7) # model HoltWinters
forecast_ets <- forecast(model_ets, h = 24 * 7) # model ets

# check MAE
MAEoneHolt <- MAE(y_pred = forecast_holt$mean, y_true = validation)
MAEoneHolt
MAEoneETS <- MAE(y_pred = forecast_ets$mean, y_true = validation)
MAEoneETS

# plot the results
plot_forecast(forecast_holt)
plot_forecast(forecast_ets)
```

We also can plot the results by including the train data.

```{r}
test_forecast(actual = fnb_ts, forecast.obj = forecast_holt,
              train = train, test = validation)
test_forecast(actual = fnb_ts, forecast.obj = forecast_ets,
              train = train, test = validation)
```

We found that the model using `ets()` function performs superior in term of MAE. Then, by looking at their plot, the model using `HoltWinters()` function tends to increase the estimated results. Instead, the other tends to be flat.

Now, let's use another model, i.e. ARIMA/SARIMA. We can use the `auto.arima()` function to find the best (by `forecast` library). However, before we employ ARIMA/SARIMA, we have to ensure that our data is stationary. We can check it by using `adf.test()` function. We expect to have p-value less than 0.05.

```{r}
adf.test(fnb_ts)
```

We have nice p-value. Now, let's make the model.

```{r}
model_arima <- auto.arima(train)
```

Then, let's see the model's summary.

```{r}
summary(model_arima)
```

In term of MAE, the model can perform small MAE. Now, let's use it to forecast the validation data.

```{r}
forecast_arima <- forecast(model_arima, h = length(validation))
MAEoneARIMA <- MAE(y_pred = forecast_arima$mean, y_true = validation)
MAEoneARIMA
plot_forecast(forecast_arima)
# ARIMA(2,0,2)(0,1,0)[168]
test_forecast(actual = fnb_ts, forecast.obj = forecast_arima,
                        train = train, test = validation)
```

Eventhough the SARIMA model results in slightly worse MAE, if we its plot, it can follow the pattern of validation data better than the two models earlier.

## Modelling of Two Seasonalities

As explained earlier that we have two types of time series object, we also consider two types of model, i.e. the model with single seasoanal and the model with multiple seasonal. We have discussed the one with single seasonal. Now, we're going to discuss the second type. As we have created the time series object for multiple seasonal, now let's split the data.

Here, we also apply the same considerations as the one with single seasonality.

```{r}
# split train-validation 2 seasonal
train_2 <- head(fnb_msts, length(fnb_msts) - 24 * 7)
validation_2 <- tail(fnb_msts, 24 * 7)
```

First, we're going to create the model using triple exponential smoothing approach.

```{r}
model_holt_msts<- HoltWinters(train_2)
forecast_holt2 <- forecast(model_holt_msts, h = 24 * 7)
MAEtwoHolt <- MAE(y_pred = forecast_holt2$mean, y_true = validation_2)
MAEtwoHolt

plot_forecast(forecast_holt2)
test_forecast(actual = fnb_msts, forecast.obj = forecast_holt2,
                        train = train_2, test = validation_2)
```

The Holt Winters model with multiple seasonality seems perform better. Its plot also show significance enhancement compared to when it only applies one seasonality.

Now, let's create the ARIMA/SARIMA model for multiple seasonalities.

```{r}
model_arima2 <- auto.arima(train_2)
```

Then, let's see the model's summary.

```{r}
summary(model_arima2)
```

```{r}
forecast_arima2 <- forecast(model_arima, h = length(validation))
MAEoneARIMA <- MAE(y_pred = forecast_arima$mean, y_true = validation)
MAEoneARIMA
plot_forecast(forecast_arima)
# ARIMA(2,0,2)(0,1,0)[168]
test_forecast(actual = fnb_ts, forecast.obj = forecast_arima,
                        train = train, test = validation)
```


###### Test dataset using one seasonal

```{r}
fnb_test <- read.csv("data/data-test.csv")


model_ready <- HoltWinters(fnb_ts, seasonal = "additive")
forecast_test <- forecast::forecast(model_ready, h = 24*7)

OpenTimeIdx <- grep("^10$", (rep(0:23, 7)))
CloseTimeIdx <- grep("^22$", (rep(0:23, 7)))

testOutput <- forecast_test$mean[c(OpenTimeIdx[1]:CloseTimeIdx[1],
  OpenTimeIdx[2]:CloseTimeIdx[2],
  OpenTimeIdx[3]:CloseTimeIdx[3],
  OpenTimeIdx[4]:CloseTimeIdx[4],
  OpenTimeIdx[5]:CloseTimeIdx[5],
  OpenTimeIdx[6]:CloseTimeIdx[6],
  OpenTimeIdx[7]:CloseTimeIdx[7])]

fnb_test$visitor <- testOutput
write.csv(fnb_test, file = "data/testOutput.csv")
```

```{r}

```

################## test dataset using 2 seasonal

```{r}
model_ready <- HoltWinters(fnb_msts)
forecast_test <- forecast::forecast(model_ready, h = 24*7)

OpenTimeIdx <- grep("^10$", (rep(0:23, 7)))
CloseTimeIdx <- grep("^22$", (rep(0:23, 7)))

testOutput <- forecast_test$mean[c(OpenTimeIdx[1]:CloseTimeIdx[1],
  OpenTimeIdx[2]:CloseTimeIdx[2],
  OpenTimeIdx[3]:CloseTimeIdx[3],
  OpenTimeIdx[4]:CloseTimeIdx[4],
  OpenTimeIdx[5]:CloseTimeIdx[5],
  OpenTimeIdx[6]:CloseTimeIdx[6],
  OpenTimeIdx[7]:CloseTimeIdx[7])]

fnb_test$visitor <- testOutput
write.csv(fnb_test, file = "data/testOutput2Season.csv")
```

```{r}
model_ready <- forecast::auto.arima(fnb_msts)
forecast_test <- forecast::forecast(model_ready, h = 24*7)
modelSarimaTest <- model_ready

testOutput <- forecast_test$mean[c(OpenTimeIdx[1]:CloseTimeIdx[1],
  OpenTimeIdx[2]:CloseTimeIdx[2],
  OpenTimeIdx[3]:CloseTimeIdx[3],
  OpenTimeIdx[4]:CloseTimeIdx[4],
  OpenTimeIdx[5]:CloseTimeIdx[5],
  OpenTimeIdx[6]:CloseTimeIdx[6],
  OpenTimeIdx[7]:CloseTimeIdx[7])]

fnb_test$visitor <- testOutput
write.csv(fnb_test, file = "data/testOutputSarima.csv")
```

```{r}
summary(modelSarimaTest)
```

################

```{r}
OpenTimeIdx <- grep("^10$", (rep(0:23, 7)))
CloseTimeIdx <- grep("^22$", (rep(0:23, 7)))
```


################

```{r}
one <- read.csv("data/testOutput.csv")
two <- read.csv("data/testOutput2Season.csv")
three <- read.csv("data/testOutputArima.csv")
four <- read.csv("data/testOutputTBATS.csv")

p1 <- data.frame(datetime = ymd_hms(one$datetime), cbind(one = one$visitor, two = two$visitor, three = three$visitor, four = four$visitor)) %>% 
  ggplot(aes(datetime)) +
  geom_line(aes(y = one), color = "blue") +
  geom_line(aes(y = two), color = "red") +
  geom_line(aes(y = three), color = "green") +
  geom_line(aes(y = four), color = "black")

plotly::ggplotly(p1)
```

##################

```{r}
tbats_mod <- fnb_msts %>%
            forecast::tbats(use.box.cox = FALSE, 
                            use.trend = TRUE, 
                            use.damped.trend = TRUE)
tbats_fore <-  forecast::forecast(tbats_mod,h=24*7)

testOutput <- tbats_fore$mean[c(OpenTimeIdx[1]:CloseTimeIdx[1],
  OpenTimeIdx[2]:CloseTimeIdx[2],
  OpenTimeIdx[3]:CloseTimeIdx[3],
  OpenTimeIdx[4]:CloseTimeIdx[4],
  OpenTimeIdx[5]:CloseTimeIdx[5],
  OpenTimeIdx[6]:CloseTimeIdx[6],
  OpenTimeIdx[7]:CloseTimeIdx[7])]

fnb_test$visitor <- testOutput
write.csv(fnb_test, file = "data/testOutputTBATS.csv")
```



